---
title: "Project1(B) Report"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project 1: Multivariate Non-Normal Distributions and Correlated Data

### Distributions and correlation

- We intend to generate longitudinal data.

- Target correlation structure is AR(1) or exchangeable correlation.

### Data generation method

```{r}
set.seed(123)  # for reproducibility

# Define sizes and parameters
n     <- 200    # number of subjects
t     <- 4      # number of repeated measurements per subject
beta0 <- -1.0   # intercept on logit scale
beta1 <-  0.3   # effect of time on logit scale
sigma <-  1.0   # std dev of random intercept (b_i)

# Create a data frame with one row per subject-time combination
# We'll store each subject's random intercept in 'b_i'.
dat <- data.frame(
  id   = rep(1:n, each = t),
  time = rep(1:t,     times = n)
)

# Simulate one random intercept per subject
# Then replicate that intercept across all time points for that subject.
b_i <- rnorm(n, mean = 0, sd = sigma)
dat$b_i <- b_i[dat$id]   # match the random intercept to each row

# Compute probability p_ij = logistic(beta0 + beta1*time_j + b_i)
# Then draw Y_ij ~ Bernoulli(p_ij).
dat$p_ij <- plogis(beta0 + beta1 * dat$time + dat$b_i)
dat$Y    <- rbinom(n * t, size = 1, prob = dat$p_ij)

head(dat)
```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

#    Simply show the binary response Y.
ggplot(dat, aes(x = factor(Y))) +
  geom_bar(width = 0.4, fill = "steelblue") +
  xlab("Outcome Y (0 or 1)") +
  ylab("Count") +
  ggtitle("Distribution of Binary Outcome (Non-Normal)")

# each row becomes a subject
dat_wide <- dat %>%
  select(id, time, Y) %>%
  pivot_wider(names_from = time,
              values_from = Y,
              names_prefix = "Y_")

# Now dat_wide has columns: id, Y_1, Y_2, ... up to Y_t
pairwise_corr <- cor(dat_wide[, -1]) 
pairwise_corr
#> This matrix shows the sample correlation among times 1..t,

# which clearly prove that outcomes are not independent
pairs(
  dat_wide[, -1] + matrix(runif(nrow(dat_wide)*(ncol(dat_wide)-1), 0, 0.1),
                          nrow(dat_wide), ncol(dat_wide)-1),
  main = "Pairs Plot (Jittered) of Y_t across times"
)



```


### Simulation study 1

```{r}
library(tidyverse)
library(ggplot2)
# --- 1. Data Generation Function ---
generate_correlated_data <- function(n = 200, t = 4, beta0 = -1.0, beta1 = 0.3, 
                                     sigma = 1.0, distribution = "binomial", df = 3) {
  dat <- data.frame(
    id   = rep(1:n, each = t),
    time = rep(1:t, times = n)
  )
  
  # Generate a random intercept for each subject (induces within-subject correlation)
  b_i <- rnorm(n, 0, sigma)
  dat$b_i <- b_i[dat$id]
  
  # Linear predictor (eta)
  eta <- beta0 + beta1 * dat$time + dat$b_i
  
  if (distribution == "binomial") {
    dat$p_ij <- plogis(eta)
    dat$Y    <- rbinom(n * t, size = 1, prob = dat$p_ij)
  } else if (distribution == "t-dist") {
    # Continuous data: add t-distributed errors with specified degrees of freedom
    errors <- rt(n * t, df = df)
    dat$Y <- eta + errors
  } else {
    stop("Unknown distribution. Choose 'binomial' or 't-dist'.")
  }
  
  return(dat)
}

# --- 2. Naive Model Fitting Function ---
fit_naive_model <- function(dat, distribution) {
  if (distribution == "binomial") {
    # For binary data -> Logistic regression
    fit <- glm(Y ~ time, data = dat, family = binomial)
    summ <- summary(fit)
    
    beta1_hat <- summ$coefficients["time", "Estimate"]
    se1       <- summ$coefficients["time", "Std. Error"]
    
    ci_lower <- beta1_hat - 1.96 * se1
    ci_upper <- beta1_hat + 1.96 * se1
    
    p_val <- summ$coefficients["time", "Pr(>|z|)"]
    
  } else if (distribution == "t-dist") {
    # For continuous data -> Linear regression
    fit <- lm(Y ~ time, data = dat)
    summ <- summary(fit)
    
    beta1_hat <- summ$coefficients["time", "Estimate"]
    se1       <- summ$coefficients["time", "Std. Error"]
    
    ci_lower <- beta1_hat - 1.96 * se1
    ci_upper <- beta1_hat + 1.96 * se1
    
    p_val <- summ$coefficients["time", "Pr(>|t|)"]
    
  } else {
    stop("Unknown distribution. Choose 'binomial' or 't-dist'.")
  }
  
  return(list(
    beta1_hat = beta1_hat,
    se1       = se1,
    ci_lower  = ci_lower,
    ci_upper  = ci_upper,
    p_val     = p_val
  ))
}

# --- 3. Single-Scenario Simulation Function ---
run_simulation <- function(n_sim = 1000, n, t, beta0, beta1, sigma, distribution, df = 3) {
  results <- data.frame(
    beta1_est = numeric(n_sim),
    se        = numeric(n_sim),
    coverage  = logical(n_sim),
    p_value   = numeric(n_sim)
  )
  
  for (i in 1:n_sim) {
    # 1) Generate data, passing df (used only for t-dist)
    dat <- generate_correlated_data(n, t, beta0, beta1, sigma, distribution, df)
    
    # 2) Fit the naive model and extract metrics
    fit <- fit_naive_model(dat, distribution)
    
    # 3) Record results
    results$beta1_est[i] <- fit$beta1_hat
    results$se[i]        <- fit$se1
    results$coverage[i]  <- (fit$ci_lower <= beta1 && fit$ci_upper >= beta1)
    results$p_value[i]   <- fit$p_val
  }
  
  return(results)
}

# --- 4. Construct Parameter Grid (including beta1 = 0 and beta1 = 0.3) ---
param_grid <- expand.grid(
  n = c(50, 200, 500),                # Sample sizes
  sigma = c(0.5, 1.0, 2.0),             # Random intercept SD (controls correlation strength)
  distribution = c("binomial", "t-dist"), # Binary (logistic) vs. heavy-tailed continuous data
  beta1_val = c(0, 0.3),                # beta1 = 0 for Type I error, beta1 = 0.3 for Power
  df = c(3, 5, 10)                     # Degrees of freedom for t-dist (ignored for binomial)
)

beta0_true <- -1
t_time     <- 4
n_sim      <- 500

all_results <- list()

# --- 5. Loop over Parameter Grid and Collect Results ---
for (i in 1:nrow(param_grid)) {
  params <- param_grid[i, ]
  
  cat("Running simulation for:",
      "n =", params$n, 
      "sigma =", params$sigma, 
      "distribution =", params$distribution,
      "beta1 =", params$beta1_val,
      "df =", params$df, "\n")
  
  sim_res <- run_simulation(
    n_sim = n_sim, 
    n     = params$n, 
    t     = t_time, 
    beta0 = beta0_true, 
    beta1 = params$beta1_val, 
    sigma = params$sigma, 
    distribution = params$distribution,
    df = params$df
  )
  
  # Calculate bias, coverage, and significance rate (sig_rate)
  bias <- mean(sim_res$beta1_est) - params$beta1_val
  coverage <- mean(sim_res$coverage)
  sig_rate <- mean(sim_res$p_value < 0.05)
  
  all_results[[i]] <- cbind(
    params,
    bias = bias,
    coverage = coverage,
    sig_rate = sig_rate
  )
}

final_results <- bind_rows(all_results)

# --- 6. Separate Result Tables for Type I Error and Power ---
type1_error_results <- final_results %>% 
  filter(beta1_val == 0) %>% 
  select(n, sigma, distribution, df, bias, coverage, sig_rate) %>% 
  rename(TypeI_error = sig_rate)

power_results <- final_results %>% 
  filter(beta1_val != 0) %>% 
  select(n, sigma, distribution, df, bias, coverage, sig_rate) %>% 
  rename(Power = sig_rate)

cat("\nResults for Type I Error (beta1 = 0):\n")
print(type1_error_results)

cat("\nResults for Power (beta1 = 0.3):\n")
print(power_results)





```
```{r}
# Filter results for t-dist only (df is only meaningful for t-dist)
t_dist_results <- final_results %>% filter(distribution == "t-dist")

# Separate the results for Type I error (beta1_val = 0) and for Power (beta1_val = 0.3)
t_dist_null <- t_dist_results %>% filter(beta1_val == 0)
t_dist_power <- t_dist_results %>% filter(beta1_val == 0.3)

# ----- Visualization for t-dist (Type I Error) -----
# Plot Bias vs. df for Type I Error (β₁ = 0) across different sigma and n
ggplot(t_dist_null, aes(x = factor(df), y = bias, color = factor(sigma))) +
  geom_point(size = 3) +
  geom_line(aes(group = sigma)) +
  facet_wrap(~ n, scales = "free_y") +
  labs(title = "Bias (Type I Error) for t-dist Data (β₁ = 0)",
       x = "Degrees of Freedom (df)",
       y = "Bias",
       color = "Sigma")

# Plot Coverage vs. df for Type I Error
ggplot(t_dist_null, aes(x = factor(df), y = coverage, color = factor(sigma))) +
  geom_point(size = 3) +
  geom_line(aes(group = sigma)) +
  facet_wrap(~ n) +
  labs(title = "Coverage (Type I Error) for t-dist Data (β₁ = 0)",
       x = "Degrees of Freedom (df)",
       y = "Coverage",
       color = "Sigma")

# Plot sig_rate vs. df for Type I Error (this is the Type I error rate)
ggplot(t_dist_null, aes(x = factor(df), y = sig_rate, color = factor(sigma))) +
  geom_point(size = 3) +
  geom_line(aes(group = sigma)) +
  facet_wrap(~ n) +
  labs(title = "Type I Error Rate for t-dist Data (β₁ = 0)",
       x = "Degrees of Freedom (df)",
       y = "Type I Error Rate",
       color = "Sigma")

# ----- Visualization for t-dist (Power) -----
# Plot Bias vs. df for Power (β₁ = 0.3)
ggplot(t_dist_power, aes(x = factor(df), y = bias, color = factor(sigma))) +
  geom_point(size = 3) +
  geom_line(aes(group = sigma)) +
  facet_wrap(~ n, scales = "free_y") +
  labs(title = "Bias (Power) for t-dist Data (β₁ = 0.3)",
       x = "Degrees of Freedom (df)",
       y = "Bias",
       color = "Sigma")

# Plot Coverage vs. df for Power
ggplot(t_dist_power, aes(x = factor(df), y = coverage, color = factor(sigma))) +
  geom_point(size = 3) +
  geom_line(aes(group = sigma)) +
  facet_wrap(~ n) +
  labs(title = "Coverage (Power) for t-dist Data (β₁ = 0.3)",
       x = "Degrees of Freedom (df)",
       y = "Coverage",
       color = "Sigma")

# Plot sig_rate vs. df for Power (this is the Power)
ggplot(t_dist_power, aes(x = factor(df), y = sig_rate, color = factor(sigma))) +
  geom_point(size = 3) +
  geom_line(aes(group = sigma)) +
  facet_wrap(~ n) +
  labs(title = "Power for t-dist Data (β₁ = 0.3)",
       x = "Degrees of Freedom (df)",
       y = "Power (sig_rate)",
       color = "Sigma")





```
```{r}
# For binomial: df is not used, so we can plot results by n and sigma.
# Visualize bias for binomial data
ggplot(binomial_results, aes(x = factor(sigma), y = bias, fill = factor(n))) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Bias for Binomial Distribution", x = "Sigma", y = "Bias", fill = "Sample Size (n)")

# Visualize coverage for binomial data
ggplot(binomial_results, aes(x = factor(sigma), y = coverage, fill = factor(n))) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Coverage for Binomial Distribution", x = "Sigma", y = "Coverage", fill = "Sample Size (n)")

# Visualize sig_rate (Type I error or Power) for binomial data
ggplot(binomial_results, aes(x = factor(sigma), y = sig_rate, fill = factor(n))) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Sig_rate for Binomial Distribution", x = "Sigma", y = "Sig_rate", fill = "Sample Size (n)")
```
### Simulation study 2

```{r}
pacman::p_load(geepack,MASS,Matrix,ggplot2, reshape2, ggcorrplot)

#--------------------------------------------------------------------------#
##### Generating the data #####
# Function to generate Longitudinal Multivariate Poisson Data
generate_longitudinal_poisson <- function(n_subjects, n_time, rho, lambda_base, beta1) {
  # Step 1: AR(1) correlation matrix
  Sigma <- outer(1:n_time, 1:n_time, function(i, j) rho^abs(i - j))
  Sigma <- as.matrix(nearPD(Sigma)$mat)  # Ensure positive definiteness
  
  # Step 2: Generate subject-specific latent normal effects
  subject_random_effects <- mvrnorm(n_subjects, mu = rep(0, n_time), Sigma = Sigma)
  
  # Step 3: Assign binary predictor (50% probability)
  subject_data <- data.frame(
    Subject = 1:n_subjects,
    X = rbinom(n_subjects, 1, 0.5)  # Binary predictor (0 or 1)
  )
  
  # Step 4: Expand into long format for longitudinal structure
  long_data <- merge(
    subject_data,
    data.frame(Time = rep(1:n_time, times = n_subjects), Subject = rep(1:n_subjects, each = n_time)),
    by = "Subject"
  )
  
  # Step 5: Assign latent normal effect to each subject-time combination
  long_data$Z <- as.vector(subject_random_effects)
  
  # Step 6: Compute Poisson mean (lambda) using log-link with covariate
  long_data$Lambda <- lambda_base * exp(beta1 * long_data$X + long_data$Z)
  
  # Step 7: Generate Poisson counts
  long_data$Count <- rpois(n_subjects * n_time, lambda = long_data$Lambda)
  
  return(long_data)
}
# Example: Generate longitudinal Poisson data for 1000 subjects over 5 time points

longitudinal_poisson_data <- generate_longitudinal_poisson(n_subjects = 100000, 
                                                           n_time = 5, 
                                                           rho = 0.5, 
                                                           lambda_base = 5, 
                                                           beta1 = 0.3)
# Print first few rows
head(longitudinal_poisson_data)

# Plot Poisson Counts Over Time
ggplot(longitudinal_poisson_data, aes(x = Time, y = Count, group = Subject, color = as.factor(X))) +
  geom_line(alpha = 0.3) +
  geom_point(alpha = 0.3) +
  labs(title = "Simulated Longitudinal Poisson Data with Predictor", x = "Time", y = "Count", color = "X (Predictor)") +
  theme_minimal()

# Compare Count Distributions for X=0 vs X=1
ggplot(longitudinal_poisson_data, aes(x = Count, fill = as.factor(X))) +
  geom_histogram(binwidth = 1, position = "dodge", alpha = 0.7) +
  scale_fill_manual(values = c("blue", "red"), labels = c("X=0", "X=1")) +
  labs(title = "Distribution of Poisson Counts by Predictor X",
       x = "Poisson Count", fill = "Predictor X") +
  theme_minimal()

# Reshape data for correlation check
cor_data <- dcast(longitudinal_poisson_data, Subject ~ Time, value.var = "Count")

# Compute correlation matrix
cor_matrix <- cor(cor_data[, -1], use = "pairwise.complete.obs")

# Plot correlation heatmap  
ggcorrplot(cor_matrix, method = "circle", type = "lower", lab = TRUE) +
  labs(title = "Correlation Between Time Points in Longitudinal Poisson Data")


#--------------------------------------------------------------------------#
##### running a linear regression #####

# Define simulation parameters
sample_sizes <- c(50, 100, 500, 1000)  # Varying sample sizes
rho_levels <- c(0, 0.3, 0.6, 0.9)  # Varying correlation levels
non_normality_levels <- c(0.1, 0.5, 1, 2, 10, 50)  # Skewness levels
n_time <- 4  # Fixed number of time points
lambda_base <- 5  # Base Poisson mean
beta1_null <- 0  # Set beta1 = 0 to compute Type I error rate
n_sim <- 100  # More replications for stable estimates

# Expand grid to create all parameter combinations
test_conditions <- expand.grid(n_subjects = sample_sizes, 
                               rho = rho_levels, 
                               non_normality_level = non_normality_levels)



# Function to fit an incorrect OLS model while varying non-normality & tracking Type I Error
simulate_ols_analysis <- function(n_subjects, n_time, rho, lambda_base, beta1, non_normality_level, n_sim) {
  type1_errors <- numeric(n_sim)
  bias_values <- numeric(n_sim)
  coverage_values <- numeric(n_sim)  # Stores whether CI contains true beta1
  
  for (i in 1:n_sim) {
    # Adjust lambda for non-normality level
    adjusted_lambda <- lambda_base / non_normality_level
    
    # Generate Poisson data
    sim_data <- generate_longitudinal_poisson(n_subjects, n_time, rho, adjusted_lambda, beta1)
    
    # Fit OLS model **(Task 3 explicitly requires incorrect OLS)**
    lm_model <- lm(Count ~ X, data = sim_data)  
    
    # Extract p-value for predictor X
    p_value <- summary(lm_model)$coefficients["X", "Pr(>|t|)"]
    type1_errors[i] <- ifelse(p_value < 0.05, 1, 0)
    
    # Compute bias
    beta1_hat <- coef(lm_model)["X"]
    bias_values[i] <- beta1_hat - beta1
    
    # Compute 95% CI for beta1
    beta1_se <- summary(lm_model)$coefficients["X", "Std. Error"]
    beta1_CI <- beta1_hat + c(-1.96, 1.96) * beta1_se
    
    # Check if CI contains true beta1
    coverage_values[i] <- (beta1_CI[1] <= beta1 & beta1_CI[2] >= beta1)
  }
  
  # Compute average Type I Error, Bias, and Coverage
  type1_error_rate <- mean(type1_errors)
  avg_bias <- mean(bias_values)
  coverage_prob <- mean(coverage_values)
  
  # Print diagnostics
  cat("n_subjects:", n_subjects, "rho:", rho, "non_normality_level:", non_normality_level, 
      "Lambda:", adjusted_lambda, "Type I Error Rate:", type1_error_rate, "Coverage:", coverage_prob, "\n")
  
  return(data.frame(
    n_subjects = n_subjects,
    rho = rho,
    non_normality_level = non_normality_level,
    adjusted_lambda = adjusted_lambda,
    avg_bias = avg_bias,
    type1_error_rate = type1_error_rate,
    coverage_prob = coverage_prob
  ))
}


# Run simulations
set.seed(123)
simulation_results_ols <- do.call(rbind, apply(test_conditions, 1, function(row) {
  simulate_ols_analysis(n_subjects = row["n_subjects"], 
                        n_time = n_time, 
                        rho = row["rho"], 
                        lambda_base = lambda_base, 
                        beta1 = beta1_null,  
                        non_normality_level = row["non_normality_level"],
                        n_sim = n_sim)  
}))

# Print results
print(simulation_results_ols)

# Plot Bias vs. Correlation
ggplot(simulation_results_ols, aes(x = rho, y = avg_bias, color = as.factor(non_normality_level))) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Bias in OLS Regression Across Correlation and Non-Normality Levels",
       x = "Correlation Level", y = "Bias",
       color = "Degree of Non-Normality") +
  theme_minimal()

# Plot Type I Error Rate vs. Correlation
ggplot(simulation_results_ols, aes(x = rho, y = type1_error_rate, color = as.factor(non_normality_level))) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Type I Error Rate in OLS Regression",
       x = "Correlation Level", y = "Type I Error Rate",
       color = "Degree of Non-Normality") +
  theme_minimal()

# Plot Coverage Probability vs. Correlation
ggplot(simulation_results_ols, aes(x = rho, y = coverage_prob, color = as.factor(non_normality_level))) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Coverage Probability of Confidence Intervals",
       x = "Correlation Level", y = "Coverage Probability",
       color = "Degree of Non-Normality") +
  theme_minimal()


#--------------------------------------------------------------------------#
##### running a GEE poisson regression #####

simulate_gee_analysis <- function(n_subjects, n_time, rho, lambda_base, beta1, non_normality_level, n_sim) {
  type1_errors <- numeric(n_sim)
  bias_values <- numeric(n_sim)
  coverage_values <- numeric(n_sim)
  
  for (i in 1:n_sim) {
    # Adjust lambda for non-normality level
    adjusted_lambda <- lambda_base / non_normality_level
    
    # Generate Poisson-distributed correlated data
    sim_data <- generate_longitudinal_poisson(n_subjects, n_time, rho, adjusted_lambda, beta1)
    
    # Fit a **correct** GEE model
    gee_model <- geeglm(Count ~ X, id = Subject, data = sim_data, family = poisson, corstr = "ar1")
    
    # Extract p-value for predictor X
    p_value <- summary(gee_model)$coefficients["X", "Pr(>|W|)"]
    type1_errors[i] <- ifelse(p_value < 0.05, 1, 0)
    
    # Compute bias
    beta1_hat <- coef(gee_model)["X"]
    bias_values[i] <- beta1_hat - beta1
    
    # Compute 95% CI
    beta1_se <- summary(gee_model)$coefficients["X", "Std.err"]
    beta1_CI <- beta1_hat + c(-1.96, 1.96) * beta1_se
    
    # Check if CI contains true beta1
    coverage_values[i] <- (beta1_CI[1] <= beta1 & beta1_CI[2] >= beta1)
  }
  
  # Compute average Type I Error, Bias, and Coverage
  type1_error_rate <- mean(type1_errors)
  avg_bias <- mean(bias_values)
  coverage_prob <- mean(coverage_values)
  
  # Print diagnostics
  cat("n_subjects:", n_subjects, "rho:", rho, "non_normality_level:", non_normality_level, 
      "Lambda:", adjusted_lambda, "Type I Error Rate (GEE):", type1_error_rate, "Coverage (GEE):", coverage_prob, "\n")
  
  return(data.frame(
    n_subjects = n_subjects,
    rho = rho,
    non_normality_level = non_normality_level,
    adjusted_lambda = adjusted_lambda,
    avg_bias = avg_bias,
    type1_error_rate = type1_error_rate,
    coverage_prob = coverage_prob
  ))
}

# Run simulations
set.seed(123)
simulation_results_gee <- do.call(rbind, apply(test_conditions, 1, function(row) {
  simulate_gee_analysis(n_subjects = row["n_subjects"], 
                        n_time = n_time, 
                        rho = row["rho"], 
                        lambda_base = lambda_base, 
                        beta1 = beta1_null,  
                        non_normality_level = row["non_normality_level"],
                        n_sim = n_sim)  
}))


# Print results
print(simulation_results_gee)


# Plot Bias vs. Correlation
ggplot(simulation_results_gee, aes(x = rho, y = avg_bias, color = as.factor(non_normality_level))) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Bias in GEE Regression Across Correlation and Non-Normality Levels",
       x = "Correlation Level", y = "Bias",
       color = "Degree of Non-Normality") +
  theme_minimal()

# Plot Type I Error Rate vs. Correlation
ggplot(simulation_results_gee, aes(x = rho, y = type1_error_rate, color = as.factor(non_normality_level))) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Type I Error Rate in GEE Regression",
       x = "Correlation Level", y = "Type I Error Rate",
       color = "Degree of Non-Normality") +
  theme_minimal()

# Plot Coverage Probability vs. Correlation
ggplot(simulation_results_gee, aes(x = rho, y = coverage_prob, color = as.factor(non_normality_level))) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Coverage Probability of Confidence Intervals",
       x = "Correlation Level", y = "Coverage Probability",
       color = "Degree of Non-Normality") +
  theme_minimal()

#--------------------------------------------------------------------------#
##### Model comparison #####

simulation_results_gee$model <- "GEE"
simulation_results_ols$model <- "OLS"

# Combine the results
comparison_results <- rbind(simulation_results_gee, simulation_results_ols)

ggplot(comparison_results, aes(x = rho, y = avg_bias, color = model, linetype = model)) +
  geom_line(size = 1) + geom_point(size = 2) +
  facet_wrap(~ n_subjects, scales = "fixed") +  
  labs(title = "Bias in OLS vs GEE Regression",
       x = "Correlation Level", y = "Bias",
       color = "Model Type", linetype = "Model Type") +
  theme_minimal()


ggplot(comparison_results, aes(x = rho, y = type1_error_rate, color = model, linetype = model)) +
  geom_line(size = 1) + geom_point(size = 2) +
  facet_wrap(~ n_subjects, scales = "fixed") +  
  labs(title = "Type I Error Rate: OLS vs GEE",
       x = "Correlation Level", y = "Type I Error Rate",
       color = "Model Type", linetype = "Model Type") +
  theme_minimal()


ggplot(comparison_results, aes(x = rho, y = coverage_prob, color = model, linetype = model)) +
  geom_line(size = 1) + geom_point(size = 2) +
  facet_wrap(~ n_subjects, scales = "fixed") +  
  labs(title = "Coverage Probability of Confidence Intervals: OLS vs GEE",
       x = "Correlation Level", y = "Coverage Probability",
       color = "Model Type", linetype = "Model Type") +
  theme_minimal()

```

