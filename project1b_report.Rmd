---
title: "Project1(B) Report"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project 1: Multivariate Non-Normal Distributions and Correlated Data

### Distributions and correlation

- We intend to generate longitudinal data.

- Target correlation structure is AR(1) or exchangeable correlation.

### Data generation method

```{r}
set.seed(123)  # for reproducibility

# Define sizes and parameters
n     <- 200    # number of subjects
t     <- 4      # number of repeated measurements per subject
beta0 <- -1.0   # intercept on logit scale
beta1 <-  0.3   # effect of time on logit scale
sigma <-  1.0   # std dev of random intercept (b_i)

# Create a data frame with one row per subject-time combination
# We'll store each subject's random intercept in 'b_i'.
dat <- data.frame(
  id   = rep(1:n, each = t),
  time = rep(1:t,     times = n)
)

# Simulate one random intercept per subject
# Then replicate that intercept across all time points for that subject.
b_i <- rnorm(n, mean = 0, sd = sigma)
dat$b_i <- b_i[dat$id]   # match the random intercept to each row

# Compute probability p_ij = logistic(beta0 + beta1*time_j + b_i)
# Then draw Y_ij ~ Bernoulli(p_ij).
dat$p_ij <- plogis(beta0 + beta1 * dat$time + dat$b_i)
dat$Y    <- rbinom(n * t, size = 1, prob = dat$p_ij)

head(dat)
```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

#    Simply show the binary response Y.
ggplot(dat, aes(x = factor(Y))) +
  geom_bar(width = 0.4, fill = "steelblue") +
  xlab("Outcome Y (0 or 1)") +
  ylab("Count") +
  ggtitle("Distribution of Binary Outcome (Non-Normal)")

# each row becomes a subject
dat_wide <- dat %>%
  select(id, time, Y) %>%
  pivot_wider(names_from = time,
              values_from = Y,
              names_prefix = "Y_")

# Now dat_wide has columns: id, Y_1, Y_2, ... up to Y_t
pairwise_corr <- cor(dat_wide[, -1]) 
pairwise_corr
#> This matrix shows the sample correlation among times 1..t,

# which clearly prove that outcomes are not independent
pairs(
  dat_wide[, -1] + matrix(runif(nrow(dat_wide)*(ncol(dat_wide)-1), 0, 0.1),
                          nrow(dat_wide), ncol(dat_wide)-1),
  main = "Pairs Plot (Jittered) of Y_t across times"
)



```


### Simulation study 1

```{r}
library(tidyverse)
library(ggplot2)

# --- 1. Data Generation Function ---
generate_correlated_data <- function(n = 200, t = 4, beta0 = -1.0, beta1 = 0.3, 
                                     sigma = 1.0, distribution = "binomial") {
  dat <- data.frame(
    id   = rep(1:n, each = t),
    time = rep(1:t, times = n)
  )
  
  # Generate a random intercept for each subject (induces within-subject correlation)
  b_i <- rnorm(n, 0, sigma)
  dat$b_i <- b_i[dat$id]
  
  # Linear predictor (eta)
  eta <- beta0 + beta1 * dat$time + dat$b_i
  
  if (distribution == "binomial") {
    dat$p_ij <- plogis(eta)
    dat$Y    <- rbinom(n * t, size = 1, prob = dat$p_ij)
  } else if (distribution == "t-dist") {
    # Continuous data: add t-distributed errors (df=3) to produce heavy tails
    df <- 3
    errors <- rt(n * t, df = df)
    dat$Y <- eta + errors
  } else {
    stop("Unknown distribution. Choose 'binomial' or 't-dist'.")
  }
  
  return(dat)
}

# --- 2. Naive Model Fitting Function ---
fit_naive_model <- function(dat, distribution) {
  if (distribution == "binomial") {
    # For binary data -> Logistic regression
    fit <- glm(Y ~ time, data = dat, family = binomial)
    summ <- summary(fit)
    
    beta1_hat <- summ$coefficients["time", "Estimate"]
    se1       <- summ$coefficients["time", "Std. Error"]
    
    ci_lower <- beta1_hat - 1.96 * se1
    ci_upper <- beta1_hat + 1.96 * se1
    
    p_val <- summ$coefficients["time", "Pr(>|z|)"]
    
  } else if (distribution == "t-dist") {
    # For continuous data -> Linear regression
    fit <- lm(Y ~ time, data = dat)
    summ <- summary(fit)
    
    beta1_hat <- summ$coefficients["time", "Estimate"]
    se1       <- summ$coefficients["time", "Std. Error"]
    
    ci_lower <- beta1_hat - 1.96 * se1
    ci_upper <- beta1_hat + 1.96 * se1
    
    p_val <- summ$coefficients["time", "Pr(>|t|)"]
    
  } else {
    stop("Unknown distribution. Choose 'binomial' or 't-dist'.")
  }
  
  return(list(
    beta1_hat = beta1_hat,
    se1       = se1,
    ci_lower  = ci_lower,
    ci_upper  = ci_upper,
    p_val     = p_val
  ))
}

# --- 3. Single-Scenario Simulation Function ---
run_simulation <- function(n_sim = 1000, n, t, beta0, beta1, sigma, distribution) {
  results <- data.frame(
    beta1_est = numeric(n_sim),
    se        = numeric(n_sim),
    coverage  = logical(n_sim),
    p_value   = numeric(n_sim)
  )
  
  for (i in 1:n_sim) {
    # 1) Generate data
    dat <- generate_correlated_data(n, t, beta0, beta1, sigma, distribution)
    
    # 2) Fit the naive model and extract metrics
    fit <- fit_naive_model(dat, distribution)
    
    # 3) Record results
    results$beta1_est[i] <- fit$beta1_hat
    results$se[i]        <- fit$se1
    results$coverage[i]  <- (fit$ci_lower <= beta1 && fit$ci_upper >= beta1)
    results$p_value[i]   <- fit$p_val
  }
  
  return(results)
}

# --- 4. Construct Parameter Grid (including beta1 = 0 and beta1 = 0.3) ---
param_grid <- expand.grid(
  n = c(50, 200, 500),                # Sample sizes
  sigma = c(0.5, 1.0, 2.0),             # Random intercept SD (controls correlation strength)
  distribution = c("binomial", "t-dist"), # Binary (logistic) vs. heavy-tailed continuous data
  beta1_val = c(0, 0.3)                # beta1 = 0 for Type I error, beta1 = 0.3 for Power
)

beta0_true <- -1
t_time     <- 4
n_sim      <- 500

all_results <- list()

# --- 5. Loop over Parameter Grid and Collect Results ---
for (i in 1:nrow(param_grid)) {
  params <- param_grid[i, ]
  
  cat("Running simulation for:",
      "n =", params$n, 
      "sigma =", params$sigma, 
      "distribution =", params$distribution,
      "beta1 =", params$beta1_val, "\n")
  
  sim_res <- run_simulation(
    n_sim = n_sim, 
    n     = params$n, 
    t     = t_time, 
    beta0 = beta0_true, 
    beta1 = params$beta1_val, 
    sigma = params$sigma, 
    distribution = params$distribution
  )
  
  # Calculate bias, coverage, and significance rate (sig_rate)
  bias <- mean(sim_res$beta1_est) - params$beta1_val
  coverage <- mean(sim_res$coverage)
  sig_rate <- mean(sim_res$p_value < 0.05)
  
  # Save results; note that when beta1_val == 0, sig_rate is Type I error,
  # when beta1_val != 0, sig_rate is Power.
  all_results[[i]] <- cbind(
    params,
    bias = bias,
    coverage = coverage,
    sig_rate = sig_rate
  )
}

final_results <- bind_rows(all_results)

# --- 6. Separate Result Tables for Type I Error and Power ---
type1_error_results <- final_results %>% filter(beta1_val == 0) %>% 
  select(n, sigma, distribution, bias, coverage, sig_rate) %>% 
  rename(TypeI_error = sig_rate)
power_results <- final_results %>% filter(beta1_val != 0) %>% 
  select(n, sigma, distribution, bias, coverage, sig_rate) %>% 
  rename(Power = sig_rate)

cat("\nResults for Type I Error (beta1 = 0):\n")
print(type1_error_results)

cat("\nResults for Power (beta1 = 0.3):\n")
print(power_results)




```










### Simulation study 2

```{r}
pacman::p_load(geepack,MASS,Matrix,ggplot2)

# Function to generate Longitudinal Multivariate Poisson Data
generate_longitudinal_poisson <- function(n_subjects, n_time, rho, lambda_base) {
  # n_subjects: Number of subjects
  # n_time: Number of time points per subject
  # rho: Desired correlation between time points (within a subject)
  # lambda_base: Baseline Poisson rate (adjust to control non-normality)
  
  # Total number of observations
  n_total <- n_subjects * n_time
  
  # Step 1: Create a correlation matrix for time points (AR-1 structure)
  Sigma <- matrix(rho, nrow = n_time, ncol = n_time)
  diag(Sigma) <- 1  # Set diagonal to 1 (self-correlation)

  # Step 2: Generate subject-specific latent normal effects
  set.seed(123)
  subject_random_effects <- mvrnorm(n_subjects, mu = rep(0, n_time), Sigma = Sigma)

  # Step 3: Expand into long format for longitudinal structure
  long_data <- data.frame(
    Subject = rep(1:n_subjects, each = n_time),
    Time = rep(1:n_time, times = n_subjects)
  )
  
  # Step 4: Assign latent normal effect to each subject-time combination
  long_data$Z <- as.vector(subject_random_effects)

  # Step 5: Compute Poisson mean (lambda) with log-link
  long_data$Lambda <- lambda_base * exp(long_data$Z)  # Log-normal transformation

  # Step 6: Generate Poisson counts
  long_data$Count <- rpois(n_total, lambda = long_data$Lambda)

  return(long_data)
}

# Example: Generate longitudinal Poisson data for 100 subjects over 5 time points
set.seed(123)
longitudinal_poisson_data <- generate_longitudinal_poisson(n_subjects = 100, n_time = 5, rho = 0.5, lambda_base = 5)

# Print first few rows
head(longitudinal_poisson_data)

# Plot trajectory for a few subjects
ggplot(longitudinal_poisson_data, aes(x = Time, y = Count, group = Subject)) +
  geom_line(alpha = 0.3) +
  geom_point(alpha = 0.3) +
  labs(title = "Simulated Longitudinal Poisson Data", x = "Time", y = "Count") +
  theme_minimal()











gee_fit <- geeglm(Y ~ time, 
                  family = binomial, 
                  id = id, 
                  corstr = "exchangeable",
                  data = dat)
summary(gee_fit)

```

