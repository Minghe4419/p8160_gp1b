---
title: "Project1(B) Report"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, fig.width = 10, fig.height = 5)
```


# Project 1: Multivariate Non-Normal Distributions and Correlated Data


## Report


## Introduction


## Simulation Study 1
Ordinary Least Squares (OLS) Model:
Ordinary Least Squares (OLS) regression is a classical approach for estimating the relationship between a continuous outcome variable and one or more predictors. The model assumes that errors (residuals) are independent and normally distributed with constant variance. Under these assumptions, OLS estimates are the Best Linear Unbiased Estimates (BLUE) of the parameters. However, when the outcome variable is count data (with inherent non-normality) and observations are correlated (e.g., repeated measures within subjects), these assumptions are violated. As a result, OLS may yield biased estimates, incorrect standard errors, and unreliable hypothesis tests.

Model Fitting: 
We applied OLS regression model, using the model:Count∼X
even though OLS assumes continuous, normally distributed errors and
independent observations. This deliberate misspecification allowed us to
observe the impact of ignoring the count nature and correlation of the
data.

Simulation Settings: 
We ran the simulation for 500 replicates under each
combination of: Sample Sizes: 100, 500, and 1000 subjects. Correlation
Levels: p,ranging from 0 to 0.9. Base Mean: λ base =5.

For each replicate, we recorded:
The estimated effect of X(beta1_hat) The bias (calculated as
beta1_hat-beta1 Whether the 95% confidence interval for beta1 contained
the true value. The p-value for the predictor X to determine if the null
hypothesis is rejected.

Results: 
Our simulation results showed that the naive OLS model produced
inconsistent performance across different conditions:

Bias: Although the overall bias in the estimated effect of X was small
on average, there were certain correlation levels (especially in
moderate to high ranges of ρ) where the bias increased noticeably.
Larger sample sizes helped stabilize the estimates somewhat, but did not
fully eliminate the spikes in bias observed at specific correlation
levels.

Type I Error Rate: Contrary to the assumption of maintaining a nominal
5% error rate, the Type I error rates often exceeded this threshold,
sometimes substantially. The lack of a monotonic pattern suggests that
ignoring within-subject correlation makes the hypothesis test results
erratic, leading to inflated false-positive conclusions.

Coverage Probability: Coverage in this setting is neither stable nor
monotonic as the correlation changes. The 95% confidence intervals for
β₁ frequently failed to capture the true value, resulting in coverage
probabilities that were lower than the nominal 95%.Coverage probability
ideally should be around 95%.


## Simulation Study 2

In the simulation study 2, we used a GEE model to account for data distribution and correlation structure. We assumed that the dependent variable \( Y_{it} \) follows a Poisson distribution:

\[
Y_{it} \sim \text{Poisson}(\lambda_{it})
\]

where \( \lambda_{it} \) is the expected count for subject \( i \) at time \( t \). We model the mean using a **log link function**:

\[
\log(\lambda_{it}) = \beta_0 + \beta_1 X_{it} + Z_{it}
\]

- \( \beta_0 \) is the **intercept**,
- \( \beta_1 \) represents the **effect of the predictor** \( X_{it} \),
- \( Z_{it} \) accounts for **subject-specific random effects**.

The within-subject correlation structure is assumed to follow an **AR(1) process**, meaning that observations closer in time are more correlated:

\[
\text{Cor}(Y_{it}, Y_{is}) = \rho^{|t - s|}
\]

where \( \rho \) represents the **correlation between measurements** taken at different time points within the same subject.

GEE estimates parameters by solving the quasi-likelihood **estimating equation**:

\[
\sum_{i=1}^{N} \mathbf{D}_i^T \mathbf{V}_i^{-1} \mathbf{U}_i = 0
\]

where \( \mathbf{V}_i \) models within-subject correlation.

We used the same set of simulation parameters as the simulation study 1.


### Result

With the sample size of 100, GEE resulted in unstable regression coefficients. As the sample size increases to 500 and 1000, the coefficients were much more stable and close to zero. There was no clear trend by correlation levels, suggesting that GEE performs consistently across varying degrees of correlation in our simulation settings.

Type 1 error rate and its corresponding coverage probability of 95% confidence intervals were unstable at the sample size of 100. For the sample size of 500 and 1000, the type 1 error rate and the coverage probability were more stable and stayed near 0.05 and 0.95, respectively.




## Model Comparisons
 
We found that in our simulated data, GEE performs better than OLS. Bias is closer to zero and more stable across varying levels of correlation in GEE, compared to OLS. With sufficient sample size (n=500 or 1000), we found that GEE result in lower type 1 error rate and thus, higher coverage probability of 95% confidence intervals. 




## Discussion














## Code


### Distributions and correlation

- We intend to generate longitudinal poisson distributed data.

- Target correlation structure is AR(1).

### Data generation method

```{r}
pacman::p_load(tidyverse, geepack, MASS, Matrix, ggplot2, reshape2, ggcorrplot, copula)

#--------------------------------------------------------------------------#
##### Generating the data #####
# Function to generate Longitudinal Multivariate Poisson Data
generate_longitudinal_poisson <- function(n_subjects, n_time, rho, lambda_base, beta1) {
  # Step 1: AR(1) correlation matrix
  Sigma <- outer(1:n_time, 1:n_time, function(i, j) rho^abs(i - j))
  Sigma <- as.matrix(nearPD(Sigma)$mat)  # Ensure positive definiteness
  
  # Step 2: Generate subject-specific latent normal effects
  subject_random_effects <- mvrnorm(n_subjects, mu = rep(0, n_time), Sigma = Sigma)
  
  # Step 3: Assign binary predictor (50% probability)
  subject_data <- data.frame(
    Subject = 1:n_subjects,
    X = rbinom(n_subjects, 1, 0.5)  # Binary predictor (0 or 1)
  )
  
  # Step 4: Expand into long format for longitudinal structure
  long_data <- merge(
    subject_data,
    data.frame(Time = rep(1:n_time, times = n_subjects), Subject = rep(1:n_subjects, each = n_time)),
    by = "Subject"
  )
  
  # Step 5: Assign latent normal effect to each subject-time combination
  long_data$Z <- as.vector(subject_random_effects)
  
  # Step 6: Compute Poisson mean (lambda) using log-link with covariate
  long_data$Lambda <- lambda_base * exp(beta1 * long_data$X + long_data$Z)
  
  # Step 7: Generate Poisson counts
  long_data$Count <- rpois(n_subjects * n_time, lambda = long_data$Lambda)
  
  return(long_data)
}

```


```{r}
# Example: Generate longitudinal Poisson data for 1000 subjects over 5 time points

longitudinal_poisson_data <- generate_longitudinal_poisson(n_subjects = 10000, 
                                                           n_time = 5, 
                                                           rho = 0.5, 
                                                           lambda_base = 5, 
                                                           beta1 = 0.3)
# Print first few rows
head(longitudinal_poisson_data)

# Plot Poisson Counts Over Time
ggplot(longitudinal_poisson_data, aes(x = Time, y = Count, group = Subject, color = as.factor(X))) +
  geom_line(alpha = 0.3) +
  geom_point(alpha = 0.3) +
  labs(title = "Simulated Longitudinal Poisson Data with Predictor", x = "Time", y = "Count", color = "X (Predictor)") +
  theme_minimal()

# Compare Count Distributions for X=0 vs X=1
ggplot(longitudinal_poisson_data, aes(x = Count, fill = as.factor(X))) +
  geom_histogram(binwidth = 1, position = "dodge", alpha = 0.7) +
  scale_fill_manual(values = c("blue", "red"), labels = c("X=0", "X=1")) +
  labs(title = "Distribution of Poisson Counts by Predictor X",
       x = "Poisson Count", fill = "Predictor X") +
  theme_minimal()

# Reshape data for correlation check
cor_data <- dcast(longitudinal_poisson_data, Subject ~ Time, value.var = "Count")

# Compute correlation matrix
cor_matrix <- cor(cor_data[, -1], use = "pairwise.complete.obs")

# Plot correlation heatmap  
ggcorrplot(cor_matrix, method = "circle", type = "lower", lab = TRUE) +
  labs(title = "Correlation Between Time Points in Longitudinal Poisson Data")
```


### Simulation study 1

```{r cache=T}
#--------------------------------------------------------------------------#
##### running a linear regression #####

# Define simulation parameters
sample_sizes <- c(100, 500, 1000)  # Varying sample sizes
rho_levels <- c(0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0)  # Varying correlation levels
lambda_bases <- c(0.01)  # Base Poisson mean
n_time <- 5  # Fixed number of time points
beta1_null <- 0  # Set beta1 = 0 to compute Type I error rate
n_sim <- 500

# Expand grid to create all parameter combinations
test_conditions <- expand.grid(n_subjects = sample_sizes, 
                               rho = rho_levels,
                               lambda_base = lambda_bases)


# Function to fit an incorrect OLS model while varying non-normality & tracking Type I Error
simulate_ols_analysis <- function(n_subjects, n_time, rho, lambda_base, beta1, n_sim) {
  type1_errors <- numeric(n_sim)
  bias_values <- numeric(n_sim)
  coverage_values <- numeric(n_sim)  # Stores whether CI contains true beta1
  
  for (i in 1:n_sim) {
    
    # Generate Poisson data
    sim_data <- generate_longitudinal_poisson(n_subjects, n_time, rho, lambda_base, beta1)
    
    # Fit OLS model **(Task 3 explicitly requires incorrect OLS)**
    lm_model <- lm(Count ~ X, data = sim_data)  
    
    # Extract p-value for predictor X
    p_value <- summary(lm_model)$coefficients["X", "Pr(>|t|)"]
    type1_errors[i] <- ifelse(p_value < 0.05, 1, 0)
    
    # Compute bias
    beta1_hat <- coef(lm_model)["X"]
    bias_values[i] <- beta1_hat - beta1
    
    # Compute 95% CI for beta1
    beta1_se <- summary(lm_model)$coefficients["X", "Std. Error"]
    beta1_CI <- beta1_hat + c(-1.96, 1.96) * beta1_se
    
    # Check if CI contains true beta1
    coverage_values[i] <- (beta1_CI[1] <= beta1 & beta1_CI[2] >= beta1)
  }
  
  # Compute average Type I Error, Bias, and Coverage
  type1_error_rate <- mean(type1_errors)
  avg_bias <- mean(bias_values)
  coverage_prob <- mean(coverage_values)
  
  # Print diagnostics
  cat("n_subjects:", n_subjects, "rho:", rho, 
      "Lambda:", lambda_base, "Type I Error Rate:", type1_error_rate, "Coverage:", coverage_prob, "\n")
  
  return(data.frame(
    n_subjects = n_subjects,
    rho = rho,
    lambda_base = lambda_base,
    avg_bias = avg_bias,
    type1_error_rate = type1_error_rate,
    coverage_prob = coverage_prob
  ))
}


# Run simulations
set.seed(123)
simulation_results_ols <- do.call(rbind, apply(test_conditions, 1, function(row) {
  simulate_ols_analysis(n_subjects = row["n_subjects"], 
                        n_time = n_time, 
                        rho = row["rho"], 
                        lambda_base = row["lambda_base"], 
                        beta1 = beta1_null,
                        n_sim = n_sim)  
}))

```

```{r}
# Plot Bias vs. Correlation
ggplot(simulation_results_ols, aes(x = rho, y = avg_bias)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Bias in OLS Regression Across Correlation",
       x = "Correlation Level", y = "Bias",
       color = "Lambda") +
  theme_minimal()

# Plot Type I Error Rate vs. Correlation
ggplot(simulation_results_ols, aes(x = rho, y = type1_error_rate)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Type I Error Rate in OLS Regression",
       x = "Correlation Level", y = "Type I Error Rate",
       color = "Lambda") +
  theme_minimal()


# Plot Coverage Probability vs. Correlation
ggplot(simulation_results_ols, aes(x = rho, y = coverage_prob)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Coverage Probability of Confidence Intervals",
       x = "Correlation Level", y = "Coverage Probability") +
  theme_minimal()

```

### Simulation study 2

```{r cache=T}
#--------------------------------------------------------------------------#
##### running a GEE poisson regression #####

simulate_gee_analysis <- function(n_subjects, n_time, rho, lambda_base, beta1, n_sim) {
  type1_errors <- numeric(n_sim)
  bias_values <- numeric(n_sim)
  coverage_values <- numeric(n_sim)
  
  for (i in 1:n_sim) {
    
    # Generate Poisson-distributed correlated data
    sim_data <- generate_longitudinal_poisson(n_subjects, n_time, rho, lambda_base, beta1)
    
    # Fit a GEE model
    gee_model <- geeglm(Count ~ X, id = Subject, data = sim_data, family = poisson, corstr = "ar1")
    
    # Extract p-value for predictor X
    p_value <- summary(gee_model)$coefficients["X", "Pr(>|W|)"]
    type1_errors[i] <- ifelse(p_value < 0.05, 1, 0)
    
    # Compute bias
    beta1_hat <- coef(gee_model)["X"]
    bias_values[i] <- beta1_hat - beta1
    
    # Compute 95% CI
    beta1_se <- summary(gee_model)$coefficients["X", "Std.err"]
    beta1_CI <- beta1_hat + c(-1.96, 1.96) * beta1_se
    
    # Check if CI contains true beta1
    coverage_values[i] <- (beta1_CI[1] <= beta1 & beta1_CI[2] >= beta1)
  }
  
  # Compute average Type I Error, Bias, and Coverage
  type1_error_rate <- mean(type1_errors)
  avg_bias <- mean(bias_values)
  coverage_prob <- mean(coverage_values)
  
  # Print diagnostics
  cat("n_subjects:", n_subjects, "rho:", rho,
      "Lambda:", lambda_base, "Type I Error Rate (GEE):", type1_error_rate, "Coverage (GEE):", coverage_prob, "\n")
  
  return(data.frame(
    n_subjects = n_subjects,
    rho = rho,
    lambda_base = lambda_base,
    avg_bias = avg_bias,
    type1_error_rate = type1_error_rate,
    coverage_prob = coverage_prob
  ))
}

# Run simulations
set.seed(123)
simulation_results_gee <- do.call(rbind, apply(test_conditions, 1, function(row) {
  simulate_gee_analysis(n_subjects = row["n_subjects"], 
                        n_time = n_time, 
                        rho = row["rho"], 
                        lambda_base = row["lambda_base"], 
                        beta1 = beta1_null,
                        n_sim = n_sim)  
}))
```

```{r}
# Plot Bias vs. Correlation
ggplot(simulation_results_gee, aes(x = rho, y = avg_bias)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Bias in GEE Regression Across Correlation",
       x = "Correlation Level", y = "Bias",
       color = "Lambda") +
  theme_minimal()

# Plot Type I Error Rate vs. Correlation
ggplot(simulation_results_gee, aes(x = rho, y = type1_error_rate)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Type I Error Rate in GEE Regression",
       x = "Correlation Level", y = "Type I Error Rate",
       color = "Lambda") +
  theme_minimal()


# Plot Coverage Probability vs. Correlation
ggplot(simulation_results_gee, aes(x = rho, y = coverage_prob)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Coverage Probability of Confidence Intervals",
       x = "Correlation Level", y = "Coverage Probability") +
  theme_minimal()
```

### Model comparisons

```{r}
##### Model comparison #####

simulation_results_gee$model <- "GEE"
simulation_results_ols$model <- "OLS"

# Combine the results
comparison_results <- rbind(simulation_results_gee, simulation_results_ols)
colnames(comparison_results)

ggplot(comparison_results, aes(x = rho, y = avg_bias, color = model, linetype = model)) +
  geom_line(linewidth = 1) + geom_point(size = 2) +
  facet_wrap(~ n_subjects, scales = "fixed") +  
  labs(title = "Bias in OLS vs GEE Regression",
       x = "Correlation Level", y = "Bias",
       color = "Model Type", linetype = "Model Type") +
  theme_minimal()

ggplot(comparison_results, aes(x = rho, y = type1_error_rate, color = model, linetype = model)) +
  geom_line(linewidth = 1) + geom_point(size = 2) +
  facet_wrap(~ n_subjects, scales = "fixed") +  
  labs(title = "Type I Error Rate: OLS vs GEE",
       x = "Correlation Level", y = "Type I Error Rate",
       color = "Model Type", linetype = "Model Type") +
  theme_minimal()

ggplot(comparison_results, aes(x = rho, y = coverage_prob, color = model, linetype = model)) +
  geom_line(linewidth = 1) + geom_point(size = 2) +
  facet_wrap(~ n_subjects, scales = "fixed") +  
  labs(title = "Coverage Probability of Confidence Intervals: OLS vs GEE",
       x = "Correlation Level", y = "Coverage Probability",
       color = "Model Type", linetype = "Model Type") +
  theme_minimal()


```

