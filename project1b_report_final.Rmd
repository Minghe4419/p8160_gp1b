---
title: "Project1(B) Report"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, fig.width = 10, fig.height = 5)
```

# Project 1: Multivariate Non-Normal Distributions and Correlated Data

## Report

### Introduction

Standard regression models assume that observations are independent and normally distributed. However, real-world data, especially in longitudinal studies, frequently violate these assumptions. Observations
collected from the same subject over time tend to be correlated, and response distributions are often non-normal, exhibiting skewness or heavy tails. Failing to account for these characteristics can lead to
biased parameter estimates, incorrect Type I error rates, and misleading confidence intervals.

To investigate the impact of these assumption violations, we designed a simulation study that generates longitudinal Poisson data with an autoregressive (AR(1)) correlation structure. We then assessed the
performance of different statistical models in analyzing this data. First, we examined Ordinary Least Squares (OLS) regression which assumes independent and normally distributed errors. Because OLS does not
account for within-subject correlation, its estimates and standard errors may be unreliable when applied to correlated count data. To address these limitations, we evaluated an alternative approach: Generalized Estimating Equations (GEE). The GEE framework provides robust standard errors and explicitly models correlation within subjects using a working correlation structure, improving efficiency in parameter
estimation.

We assess bias, Type I error rates, and coverage probabilities of confidence intervals under varying levels of within-subject correlation. By highlighting the limitations of naive regression models and the
advantages of GEE, our study provides insights into the appropriate statistical methods for handling correlated, non-normal data in longitudinal research.

To address these limitations, we evaluated an alternative approach: **Generalized Estimating Equations (GEE)**. GEE extends generalized linear models by incorporating a **working covariance matrix**, which accounts for the correlation between repeated measures. Unlike maximum likelihood estimation, which relies on a fully specified likelihood function, GEE instead solves an **estimating equation** for the regression parameters. This is done using an **iterative Newton-Raphson-like approach**, where the parameter estimates are updated using the covariance-adjusted score function:

$$
\hat{\beta}_{r+1} = \hat{\beta}_{r} + 
\left[
\sum_{i=1}^{n} D_i^T V_i^{-1} D_i
\right]^{-1} 
\left[
\sum_{i=1}^{n} D_i^T V_i^{-1} s_i
\right]_{\beta=\hat{\beta}_r}
$$

### Copula transformation to generate correlation structure

Copula transformation is tested in its ability to generate correlation structure in our data.

The histograms of the two data show that the Gaussian copula transformed data resemble the data generated by direct multivariate normal approach. Overall, the shape of the two distributions are similar. The box plots also show that the mean and variability of the two distributions are comparable.

The results are shown. OLS is a linear model that is not well suited for Poisson-distributed outcomes. Its inefficiency in modeling such non-normal data can lead to higher variability (i.e., larger standard errors) in the estimates. GEE explicitly models the within‐subject correlation. When the working correlation structure (here, AR(1)) is correctly specified, GEE “borrows strength” from the repeated measurements by accounting for their redundancy and outperform the mis-specified correlation structure model.

### Simulation Study 1

Ordinary Least Squares (OLS) Model:

Ordinary Least Squares (OLS) regression is a classical approach for estimating the relationship between a continuous outcome variable and one or more predictors. The model assumes that errors (residuals) are
independent and normally distributed with constant variance. Under these assumptions, OLS estimates are the Best Linear Unbiased Estimates (BLUE) of the parameters.

Model Fitting:

In the first simulation study, we deliberately fit a naive Ordinary Least Squares (OLS) model to count data that were actually generated from a Poisson process with an AR(1) correlation structure. Specifically, we assumed: 
$$
Y_{ij} = \beta_0 + \beta_1 X_{ij} + \varepsilon_{ij}, \quad \varepsilon_{ij} \sim N(0, \sigma^2)
$$ 
even though the true data-generating mechanism was a Poisson distribution with correlation among repeated measures. By ignoring both the count nature and within-subject correlation of $Y{ij}$we aimed to investigate how model misspecification impacts:

-   Bias in estimating $β{1}$.

-   Type I error rate when testing$H_0: \beta_1 = 0$

-   Coverage probability of 95% confidence intervals for $β{1}$.

Simulation Settings:

We ran the simulation for 500 replicates under each combination of: 
Sample Sizes: 100, 500, and 1000 subjects. Correlation Levels: p,ranging from 0 to 0.9. Base Mean: λ base =5.

Results:

Our simulation results showed that the naive OLS model produced inconsistent performance across different conditions:

Bias:

Although the overall bias in the estimated effect of X was small on average, there were certain correlation levels (especially in moderate to high ranges of ρ) where the bias increased noticeably. Larger sample
sizes helped stabilize the estimates somewhat, but did not fully eliminate the spikes in bias observed at specific correlation levels.

Type I Error Rate:

Contrary to the assumption of maintaining a nominal 5% error rate, the Type I error rates often exceeded this threshold, sometimes substantially. The lack of a monotonic pattern suggests that ignoring within-subject correlation makes the hypothesis test results erratic, leading to inflated false-positive conclusions.

Coverage Probability:

Coverage in this setting is neither stable nor monotonic as the correlation changes. The 95% confidence intervals for β₁ frequently failed to capture the true value, resulting in coverage probabilities
that were lower than the nominal 95%.Coverage probability ideally should be around 95%.

### Simulation Study 2

In the simulation study 2, we used a GEE model to account for data distribution and correlation structure. We assumed that the dependent variable $Y_{it}$ follows a Poisson distribution:

$$
Y_{it} \sim \text{Poisson}(\lambda_{it})
$$

where $\lambda_{it}$ is the expected count for subject $i$ at time $t$. We model the mean using a **log link function**:

$$
\log(\lambda_{it}) = \beta_0 + \beta_1 X_{it} + Z_{it}
$$

-   $\beta_0$ is the **intercept**,
-   $\beta_1$ represents the **effect of the predictor** $X_{it}$,
-   $Z_{it}$ accounts for **subject-specific random effects**.

The within-subject correlation structure is assumed to follow an **AR(1) process**, meaning that observations closer in time are more correlated:

$$
\text{Cor}(Y_{it}, Y_{is}) = \rho^{|t - s|}
$$

where $\rho$ represents the **correlation between measurements** taken at different time points within the same subject.

GEE estimates parameters by solving the quasi-likelihood **estimating equation**:

$$
\sum_{i=1}^{N} \mathbf{D}_i^T \mathbf{V}_i^{-1} \mathbf{U}_i = 0
$$

where $\mathbf{V}_i$ models within-subject correlation.

We used the same set of simulation parameters as the simulation study 1.

#### Result

With the sample size of 100, GEE resulted in unstable regression coefficients. As the sample size increases to 500 and 1000, the coefficients were much more stable and close to zero. There was no clear trend by correlation levels, suggesting that GEE performs consistently across varying degrees of correlation in our simulation settings.

Type 1 error rate and its corresponding coverage probability of 95% confidence intervals were unstable at the sample size of 100. For the sample size of 500 and 1000, the type 1 error rate and the coverage probability were more stable and stayed near 0.05 and 0.95, respectively.

### Model Comparisons

We found that in our simulated data, GEE performs better than OLS. Bias is closer to zero and more stable across varying levels of correlation in GEE, compared to OLS. With sufficient sample size (n=500 or 1000), we
found that GEE result in lower type 1 error rate and thus, higher coverage probability of 95% confidence intervals.

### Conclusion

#### Non-appropriateness of OLS for Longitudinal Data:

##### Bias

Both OLS and GEE might yield an average bias near zero if the estimator is consistent, but there are some reasons why the bias from GEE remains consistently around zero while the bias from OLS tends to vary more. The OLS estimator is less efficient when applied to longitudinal count data because it ignores the within-subject correlation. This inefficiency introduces more random variation in the coefficient estimates from one simulation to another—even though on average the bias might cancel out.

##### Type I Error

As illustrated in `Introduction`, we expect the Standard Error(SE) of OLS to be lower than the SE of GEE model because of its overestimation of effective sample size. Here we compare the Type I Error between OLS and GEE models because the Type I Error directly illustrates the OLS would reject more null hypothesis when estimating the parameters than it should.
From the plot, we can see when number of simulation is at 500, the OLS model has a higher Type I Error at most of the correlation levels. And the reason we observe the higher Type I Error of GEE model at two given correlation level might because the finite‐sample issues in the GEE’s robust (sandwich) variance estimator at moderate number of clusters.[a]

##### Coverage probability of Confidence Interval

Coverage Probability of Confidence Interval is observed to have a inherently inverse relationship with the Type I Error for both models and all numbers of simulations(highest Type I Error is the lowest Converge Probability of CI at given correlation level for both models). Such inverse relationship can also be explained by underestimation of SE. When the standard errors are underestimated, the resulting confidence intervals become too narrow. This means they don’t “cover” (i.e., include) the true parameter as often as they should—leading to lower coverage probability. And underestimated SE would inflate the Type I Error as explained. Therefore, we observe such inherently inverse relationship.

In summary, the simulation results align perfectly with our theoretical expectations: OLS underestimates standard errors, leading to inflated Type I error rates and narrower confidence intervals with lower coverage, while GEE provides robust, unbiased estimates with appropriate uncertainty. This consistency across bias, Type I error, and coverage probability confirms the validity of our simulation framework and underscores the importance of using GEE for longitudinal data analysis.

### Discussion

#### Choosing the Correct Working Correlation Structure

In GEE, the working correlation structure is crucial because it determines how the model accounts for the within‐subject dependency.

Correct specification ensures that redundant information among repeated measures is properly accounted for, leading to accurate standard errors, valid p-values, and controlled Type I error rates.
Selection Criteria:

Several criteria have been developed to help choose the optimal working correlation structure, including:
QIC (Quasi-likelihood Information Criterion); CIC (Correlation Information Criterion); RJ (Rotnitzky–Jewell Criterion); PAC (Proposed Approximate Criterion) For example, the paper by Pardo and Alonso (2019) introduces PAC, which compares the model-implied covariance matrix (from the assumed working correlation) with an empirical covariance estimator (calculated from residuals) by using the determinants of these matrices. A lower PAC value indicates a closer match to the true correlation structure.

_Practical Recommendation:_

When analyzing longitudinal data, practitioners should routinely assess the working correlation using these criteria. Doing so maximizes efficiency and ensures that the model makes the best use of available information.


#### GEE vs GLMM

GEE (Generalized Estimating Equations):

- Inference Target: Provides population-average (or marginal) estimates.
Answers questions like: “What is the average effect of this predictor across the entire population?”

- Use Case: Ideal for public health, policy studies, or other applications where the focus is on the overall trend rather than individual variation.

GLMM (Generalized Linear Mixed Models):

- Inference Target: Provides subject-specific (or conditional) estimates by incorporating both fixed and random effects. Subject-specific estimates could be summed to represent marginal estimates when estimates are collapsible. In addition, if the variability of subject-specific estimates can be estimated.
Answers questions like: “What is the effect of this predictor on the outcome for a given individual, after accounting for their unique baseline characteristics?”

- Use Case: Preferable when individual predictions or understanding the variance between subjects is important.

- Practical Recommendation: Choose GEE when the goal is to understand average trends and when robust, population-level inference is desired.

Opt for GLMM when subject-level predictions and insights into individual variability are required.

#### Efficiency & Valid Inference:

A correctly specified GEE model uses criteria like QIC, CIC, RJ, and PAC to capture the true within-subject correlation structure. This leads to more accurate standard errors and reliable hypothesis tests, whereas OLS (or mis-specified models) tends to underestimate uncertainty.

### Future Directions

For complex longitudinal studies, further refinement of correlation structure selection using these criteria can enhance model performance and reliability. Integrating these methods with advanced predictive analytics may also extend their practical utility in diverse research fields.

[a] Pan, Wei. “Small‐sample adjustments in using the sandwich variance estimator in generalized estimating equations.” Statistics in medicine, vol. 21, no. 10, 05/2002, pp. 1429-1441,  doi:10.1002/sim.1142.


## Code

### Distributions and correlation

-   We intend to generate longitudinal poisson distributed data.

-   Target correlation structure is AR(1).

### Data generation method

```{r}
pacman::p_load(tidyverse, geepack, MASS, Matrix, ggplot2, reshape2, ggcorrplot, copula)

#--------------------------------------------------------------------------#
##### Generating the data #####
# Function to generate Longitudinal Multivariate Poisson Data
generate_longitudinal_poisson <- function(n_subjects, n_time, rho, lambda_base, beta1) {
  # Step 1: AR(1) correlation matrix
  Sigma <- outer(1:n_time, 1:n_time, function(i, j) rho^abs(i - j))
  Sigma <- as.matrix(nearPD(Sigma)$mat)  # Ensure positive definiteness
  
  # Step 2: Generate subject-specific latent normal effects
  subject_random_effects <- mvrnorm(n_subjects, mu = rep(0, n_time), Sigma = Sigma)
  
  # Step 3: Assign binary predictor (50% probability)
  subject_data <- data.frame(
    Subject = 1:n_subjects,
    X = rbinom(n_subjects, 1, 0.5)  # Binary predictor (0 or 1)
  )
  
  # Step 4: Expand into long format for longitudinal structure
  long_data <- merge(
    subject_data,
    data.frame(Time = rep(1:n_time, times = n_subjects), Subject = rep(1:n_subjects, each = n_time)),
    by = "Subject"
  )
  
  # Step 5: Assign latent normal effect to each subject-time combination
  long_data$Z <- as.vector(subject_random_effects)
  
  # Step 6: Compute Poisson mean (lambda) using log-link with covariate
  long_data$Lambda <- lambda_base * exp(beta1 * long_data$X + long_data$Z)
  
  # Step 7: Generate Poisson counts
  long_data$Count <- rpois(n_subjects * n_time, lambda = long_data$Lambda)
  
  return(long_data)
}

```

```{r}
# Example: Generate longitudinal Poisson data for 1000 subjects over 5 time points

longitudinal_poisson_data <- generate_longitudinal_poisson(n_subjects = 10000, 
                                                           n_time = 5, 
                                                           rho = 0.5, 
                                                           lambda_base = 5, 
                                                           beta1 = 0.3)
# Print first few rows
head(longitudinal_poisson_data)

# Plot Poisson Counts Over Time
ggplot(longitudinal_poisson_data, aes(x = Time, y = Count, group = Subject, color = as.factor(X))) +
  geom_line(alpha = 0.3) +
  geom_point(alpha = 0.3) +
  labs(title = "Simulated Longitudinal Poisson Data with Predictor", x = "Time", y = "Count", color = "X (Predictor)") +
  theme_minimal()

# Compare Count Distributions for X=0 vs X=1
ggplot(longitudinal_poisson_data, aes(x = Count, fill = as.factor(X))) +
  geom_histogram(binwidth = 1, position = "dodge", alpha = 0.7) +
  scale_fill_manual(values = c("blue", "red"), labels = c("X=0", "X=1")) +
  labs(title = "Distribution of Poisson Counts by Predictor X",
       x = "Poisson Count", fill = "Predictor X") +
  theme_minimal()

# Reshape data for correlation check
cor_data <- dcast(longitudinal_poisson_data, Subject ~ Time, value.var = "Count")

# Compute correlation matrix
cor_matrix <- cor(cor_data[, -1], use = "pairwise.complete.obs")

# Plot correlation heatmap  
ggcorrplot(cor_matrix, method = "circle", type = "lower", lab = TRUE) +
  labs(title = "Correlation Between Time Points in Longitudinal Poisson Data")
```


### Copula transformation to generate correlation

```{r cache=T}
# Function to generate Poisson data using a Gaussian copula with AR(1)
generate_poisson_copula_AR1 <- function(n, t, lambda, rho) {
  gauss_cop <- normalCopula(param = rho, dim = t, dispstr = "ar1")  # AR(1) copula
  copula_data <- rCopula(n, gauss_cop)  # Generate uniform(0,1) correlated data
  poisson_data <- matrix(qpois(copula_data, lambda), nrow = n, ncol = t)  # Convert to Poisson
  data.frame(subject = rep(1:n, each = t), time = rep(1:t, times = n), count = as.vector(poisson_data))
}
# Function to generate Poisson data using direct AR(1) correlation (without copula)
generate_poisson_direct_AR1 <- function(n, t, lambda, rho) {
  Sigma <- outer(1:t, 1:t, function(i, j) rho^abs(i - j))  # AR(1) covariance matrix
  normal_data <- mvrnorm(n = n, mu = rep(0, t), Sigma = Sigma)  # Multivariate normal data
  uniform_data <- pnorm(normal_data)  # Transform normal to uniform(0,1)
  poisson_data <- matrix(qpois(uniform_data, lambda), nrow = n, ncol = t)  # Convert to Poisson
  data.frame(subject = rep(1:n, each = t), time = rep(1:t, times = n), count = as.vector(poisson_data))
}
# Simulation parameters
set.seed(123)
n <- 100    # Number of subjects
t <- 5      # Time points per subject
lambda <- 3 # Poisson mean
rho <- 0.6  # AR(1) correlation coefficient
# Generate data using both methods
data_copula <- generate_poisson_copula_AR1(n, t, lambda, rho)
data_direct <- generate_poisson_direct_AR1(n, t, lambda, rho)
# Combine datasets for visualization
data_copula$method <- "Copula-based"
data_direct$method <- "Direct AR(1)"
combined_data <- rbind(data_copula, data_direct)
# Plot histograms
ggplot(combined_data, aes(x = count, fill = method)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 30) +
  theme_minimal() +
  labs(title = "Comparison of Poisson Distributions", x = "Poisson Count", y = "Frequency")
# Boxplot comparison
ggplot(combined_data, aes(x = method, y = count, fill = method)) +
  geom_boxplot(alpha = 0.7) +
  theme_minimal() +
  labs(title = "Boxplot Comparison of Poisson Counts", x = "Method", y = "Poisson Count")

```

```{r cache=T}
simulate_comparison <- function(n_subjects, n_time, rho, lambda, beta1, n_sim) {
  # Initialize storage vectors for estimates and SEs
  ols_est <- numeric(n_sim)
  ols_se <- numeric(n_sim)
  gee_ar1_est <- numeric(n_sim)
  gee_ar1_se <- numeric(n_sim)
  gee_exch_est <- numeric(n_sim)
  gee_exch_se <- numeric(n_sim)
  # We’ll store the model objects from the last iteration
  last_model_ols <- NULL
  last_model_gee_ar1 <- NULL
  for(i in 1:n_sim) {
    # Generate data using the direct AR(1) method
    sim_data <- generate_poisson_direct_AR1(n_subjects, n_time, lambda, rho)
    # Fit OLS model (ignoring correlation)
    # Here we add predictor X to sim_data: X is a subject-level binary variable.
    sim_data <- sim_data %>% mutate(X = rep(rbinom(n_subjects, 1, 0.5), each = n_time))
    last_model_ols <- lm(count ~ X, data = sim_data)
    ols_est[i] <- coef(last_model_ols)['X']
    ols_se[i] <- summary(last_model_ols)$coefficients['X', 'Std. Error']
    # For GEE, we need the predictor X at the subject level.
    # Simulate subject-level X and merge into sim_data.
    subject_data <- data.frame(subject = 1:n_subjects, X = rbinom(n_subjects, 1, 0.5))
    sim_data <- merge(sim_data, subject_data, by = 'subject', suffixes = c('','_sub'))
    # We’ll use the subject-level predictor (X_sub) for consistency.
    # Fit GEE with AR(1) working correlation (correctly specified when rho != 0)
    last_model_gee_ar1 <- geeglm(count ~ X_sub, id = subject, data = sim_data,
                                 family = poisson, corstr = 'ar1')
    gee_ar1_est[i] <- coef(last_model_gee_ar1)['X_sub']
    gee_ar1_se[i] <- summary(last_model_gee_ar1)$coefficients['X_sub', 'Std.err']
    # Fit GEE with exchangeable working correlation (mis-specified for AR(1) data)
    model_gee_exch <- geeglm(count ~ X_sub, id = subject, data = sim_data,
                             family = poisson, corstr = 'exchangeable')
    gee_exch_est[i] <- coef(model_gee_exch)['X_sub']
    gee_exch_se[i] <- summary(model_gee_exch)$coefficients['X_sub', 'Std.err']
  }
  # Compute average bias and average standard error for each model
  result <- data.frame(
    Model = c("OLS", "GEE_AR1", "GEE_Exchangeable"),
    Avg_Bias = c(mean(ols_est - beta1), mean(gee_ar1_est - beta1), mean(gee_exch_est - beta1)),
    Avg_SE = c(mean(ols_se), mean(gee_ar1_se), mean(gee_exch_se))
  )
  # Return a list with the summary result and the final models from the last iteration
  return(list(result = result, model_ols = last_model_ols, model_gee_ar1 = last_model_gee_ar1))
}
# Example usage:
beta1 <- 0.3
n_sim <- 500
n_subjects <- 100    # Number of subjects
n_time <- 5      # Time points per subject
result_AR1 <- simulate_comparison(n_subjects = 200, n_time = 5, rho = 0.75, lambda = 3, beta1, n_sim)
result_AR1$Data <- "AR1 Correlated"
# For independent data (rho = 0)
result_indep <- simulate_comparison(n_subjects = 200, n_time = 5, rho = 0, lambda = 3, beta1, n_sim)
result_indep$Data <- "Independent"
# Combine results for summary table:
comparison_results <- rbind(result_AR1$result, result_indep$result)
print(comparison_results)
```


### Simulation study 1

```{r cache=T}
#--------------------------------------------------------------------------#
##### running a linear regression #####

# Define simulation parameters
sample_sizes <- c(100, 500, 1000)  # Varying sample sizes
rho_levels <- c(0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0)  # Varying correlation levels
lambda_bases <- c(0.01)  # Base Poisson mean
n_time <- 5  # Fixed number of time points
beta1_null <- 0  # Set beta1 = 0 to compute Type I error rate
n_sim <- 500

# Expand grid to create all parameter combinations
test_conditions <- expand.grid(n_subjects = sample_sizes, 
                               rho = rho_levels,
                               lambda_base = lambda_bases)


# Function to fit an incorrect OLS model while varying non-normality & tracking Type I Error
simulate_ols_analysis <- function(n_subjects, n_time, rho, lambda_base, beta1, n_sim) {
  type1_errors <- numeric(n_sim)
  bias_values <- numeric(n_sim)
  coverage_values <- numeric(n_sim)  # Stores whether CI contains true beta1
  
  for (i in 1:n_sim) {
    
    # Generate Poisson data
    sim_data <- generate_longitudinal_poisson(n_subjects, n_time, rho, lambda_base, beta1)
    
    # Fit OLS model **(Task 3 explicitly requires incorrect OLS)**
    lm_model <- lm(Count ~ X, data = sim_data)  
    
    # Extract p-value for predictor X
    p_value <- summary(lm_model)$coefficients["X", "Pr(>|t|)"]
    type1_errors[i] <- ifelse(p_value < 0.05, 1, 0)
    
    # Compute bias
    beta1_hat <- coef(lm_model)["X"]
    bias_values[i] <- beta1_hat - beta1
    
    # Compute 95% CI for beta1
    beta1_se <- summary(lm_model)$coefficients["X", "Std. Error"]
    beta1_CI <- beta1_hat + c(-1.96, 1.96) * beta1_se
    
    # Check if CI contains true beta1
    coverage_values[i] <- (beta1_CI[1] <= beta1 & beta1_CI[2] >= beta1)
  }
  
  # Compute average Type I Error, Bias, and Coverage
  type1_error_rate <- mean(type1_errors)
  avg_bias <- mean(bias_values)
  coverage_prob <- mean(coverage_values)
  
  # Print diagnostics
  cat("n_subjects:", n_subjects, "rho:", rho, 
      "Lambda:", lambda_base, "Type I Error Rate:", type1_error_rate, "Coverage:", coverage_prob, "\n")
  
  return(data.frame(
    n_subjects = n_subjects,
    rho = rho,
    lambda_base = lambda_base,
    avg_bias = avg_bias,
    type1_error_rate = type1_error_rate,
    coverage_prob = coverage_prob
  ))
}


# Run simulations
set.seed(123)
simulation_results_ols <- do.call(rbind, apply(test_conditions, 1, function(row) {
  simulate_ols_analysis(n_subjects = row["n_subjects"], 
                        n_time = n_time, 
                        rho = row["rho"], 
                        lambda_base = row["lambda_base"], 
                        beta1 = beta1_null,
                        n_sim = n_sim)  
}))

```

```{r}
# Plot Bias vs. Correlation
ggplot(simulation_results_ols, aes(x = rho, y = avg_bias)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Bias in OLS Regression Across Correlation",
       x = "Correlation Level", y = "Bias",
       color = "Lambda") +
  theme_minimal()

# Plot Type I Error Rate vs. Correlation
ggplot(simulation_results_ols, aes(x = rho, y = type1_error_rate)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Type I Error Rate in OLS Regression",
       x = "Correlation Level", y = "Type I Error Rate",
       color = "Lambda") +
  theme_minimal()


# Plot Coverage Probability vs. Correlation
ggplot(simulation_results_ols, aes(x = rho, y = coverage_prob)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Coverage Probability of Confidence Intervals",
       x = "Correlation Level", y = "Coverage Probability") +
  theme_minimal()

```

### Simulation study 2

```{r cache=T}
#--------------------------------------------------------------------------#
##### running a GEE poisson regression #####

simulate_gee_analysis <- function(n_subjects, n_time, rho, lambda_base, beta1, n_sim) {
  type1_errors <- numeric(n_sim)
  bias_values <- numeric(n_sim)
  coverage_values <- numeric(n_sim)
  
  for (i in 1:n_sim) {
    
    # Generate Poisson-distributed correlated data
    sim_data <- generate_longitudinal_poisson(n_subjects, n_time, rho, lambda_base, beta1)
    
    # Fit a GEE model
    gee_model <- geeglm(Count ~ X, id = Subject, data = sim_data, family = poisson, corstr = "ar1")
    
    # Extract p-value for predictor X
    p_value <- summary(gee_model)$coefficients["X", "Pr(>|W|)"]
    type1_errors[i] <- ifelse(p_value < 0.05, 1, 0)
    
    # Compute bias
    beta1_hat <- coef(gee_model)["X"]
    bias_values[i] <- beta1_hat - beta1
    
    # Compute 95% CI
    beta1_se <- summary(gee_model)$coefficients["X", "Std.err"]
    beta1_CI <- beta1_hat + c(-1.96, 1.96) * beta1_se
    
    # Check if CI contains true beta1
    coverage_values[i] <- (beta1_CI[1] <= beta1 & beta1_CI[2] >= beta1)
  }
  
  # Compute average Type I Error, Bias, and Coverage
  type1_error_rate <- mean(type1_errors)
  avg_bias <- mean(bias_values)
  coverage_prob <- mean(coverage_values)
  
  # Print diagnostics
  cat("n_subjects:", n_subjects, "rho:", rho,
      "Lambda:", lambda_base, "Type I Error Rate (GEE):", type1_error_rate, "Coverage (GEE):", coverage_prob, "\n")
  
  return(data.frame(
    n_subjects = n_subjects,
    rho = rho,
    lambda_base = lambda_base,
    avg_bias = avg_bias,
    type1_error_rate = type1_error_rate,
    coverage_prob = coverage_prob
  ))
}

# Run simulations
set.seed(123)
simulation_results_gee <- do.call(rbind, apply(test_conditions, 1, function(row) {
  simulate_gee_analysis(n_subjects = row["n_subjects"], 
                        n_time = n_time, 
                        rho = row["rho"], 
                        lambda_base = row["lambda_base"], 
                        beta1 = beta1_null,
                        n_sim = n_sim)  
}))
```

```{r}
# Plot Bias vs. Correlation
ggplot(simulation_results_gee, aes(x = rho, y = avg_bias)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Bias in GEE Regression Across Correlation",
       x = "Correlation Level", y = "Bias",
       color = "Lambda") +
  theme_minimal()

# Plot Type I Error Rate vs. Correlation
ggplot(simulation_results_gee, aes(x = rho, y = type1_error_rate)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Type I Error Rate in GEE Regression",
       x = "Correlation Level", y = "Type I Error Rate",
       color = "Lambda") +
  theme_minimal()


# Plot Coverage Probability vs. Correlation
ggplot(simulation_results_gee, aes(x = rho, y = coverage_prob)) +
  geom_line() + geom_point() +
  facet_wrap(~ n_subjects, scales = "fixed") +
  labs(title = "Coverage Probability of Confidence Intervals",
       x = "Correlation Level", y = "Coverage Probability") +
  theme_minimal()
```

### Model comparisons

```{r}
##### Model comparison #####

simulation_results_gee$model <- "GEE"
simulation_results_ols$model <- "OLS"

# Combine the results
comparison_results <- rbind(simulation_results_gee, simulation_results_ols)
colnames(comparison_results)

ggplot(comparison_results, aes(x = rho, y = avg_bias, color = model, linetype = model)) +
  geom_line(linewidth = 1) + geom_point(size = 2) +
  facet_wrap(~ n_subjects, scales = "fixed") +  
  labs(title = "Bias in OLS vs GEE Regression",
       x = "Correlation Level", y = "Bias",
       color = "Model Type", linetype = "Model Type") +
  theme_minimal()

ggplot(comparison_results, aes(x = rho, y = type1_error_rate, color = model, linetype = model)) +
  geom_line(linewidth = 1) + geom_point(size = 2) +
  facet_wrap(~ n_subjects, scales = "fixed") +  
  labs(title = "Type I Error Rate: OLS vs GEE",
       x = "Correlation Level", y = "Type I Error Rate",
       color = "Model Type", linetype = "Model Type") +
  theme_minimal()

ggplot(comparison_results, aes(x = rho, y = coverage_prob, color = model, linetype = model)) +
  geom_line(linewidth = 1) + geom_point(size = 2) +
  facet_wrap(~ n_subjects, scales = "fixed") +  
  labs(title = "Coverage Probability of Confidence Intervals: OLS vs GEE",
       x = "Correlation Level", y = "Coverage Probability",
       color = "Model Type", linetype = "Model Type") +
  theme_minimal()


```
